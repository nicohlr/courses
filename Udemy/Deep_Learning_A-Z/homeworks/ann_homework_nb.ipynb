{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning A-Z : Building an ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is my response to the first homework of the course called *Deep Learning A-Zâ„¢: Hands-On Artificial Neural Networks* accessible here : https://www.udemy.com/deeplearning/\n",
    "\n",
    "In this notebook, we are going to build an ANN using keras and by following instructions given on the course. This neurals network will predict, for a customer of a bank, if this customer is going to leave the bank or not. We are going to train our ann with a dataset containing data about approximately 10000 clients, which also includes a response column in which we can see whether the client stayed or not in the bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_train = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))), 'ressources/Artificial_Neural_Networks/Churn_Modelling.csv')\n",
    "dataset = pd.read_csv(path_train)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      "RowNumber          10000 non-null int64\n",
      "CustomerId         10000 non-null int64\n",
      "Surname            10000 non-null object\n",
      "CreditScore        10000 non-null int64\n",
      "Geography          10000 non-null object\n",
      "Gender             10000 non-null object\n",
      "Age                10000 non-null int64\n",
      "Tenure             10000 non-null int64\n",
      "Balance            10000 non-null float64\n",
      "NumOfProducts      10000 non-null int64\n",
      "HasCrCard          10000 non-null int64\n",
      "IsActiveMember     10000 non-null int64\n",
      "EstimatedSalary    10000 non-null float64\n",
      "Exited             10000 non-null int64\n",
      "dtypes: float64(2), int64(9), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Exited\" is our response column, its the 14th column, we are going to need this information to modify the template given by the course to make it works for this case. The three first columns have no impact on the response so we will not include it in out training table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the course, we are going to preprocess our data using the template given in the course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([608, 'Spain', 'Female', 41, 1, 83807.86, 1, 0, 1, 112542.58],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:, 3:13].values # we modify indexes according to what we saw with the info() method of the dataset\n",
    "y = dataset.iloc[:, 13].values # idem\n",
    "X[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to encode our categorical variables. We are going to do it using the template of the course. Here we have two categorical columns (Geography and Gender) so we have to create to encoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "labelencoder_X_geo = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_geo.fit_transform(X[:, 1])\n",
    "labelencoder_X_gender = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_gender.fit_transform(X[:, 2])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data are now preprocessed ! We can start building our model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Let's build our ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your classifier here\n",
    "classifier = Sequential() # Initializing our ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, here are all the steps we must follow for training an ANN with stochastic gradient descent method. Dense function will be used for step 1. From step 2, we know that each features is attributed to one node, so we have to create 11 input nodes in our input layer. \n",
    "We also have to choose an activation function (step 3) as we saw in the course, we will use the best one for our hidden layers (based on experiment) : the rectifier function. The sigmoid function is a very good option for our output layer because it will gives us probabilities for each classes.\n",
    "After that, we will use learning rate to choose how weights are updated and we will also think about how many epochs we are going to do. Let's go !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/steps.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the first hidden layer of our ANN\n",
    "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_shape = (11,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *units* param corresponds to the number of node of the layer:\n",
    "\n",
    "**tip :** choose the number of nodes in the hidden layers as the average of the number of nodes in the input layer and the number of nodes in the output layer\n",
    "Here we have 11 nodes in input layer and 1 node in output layer (because binary output) so we choose 6 nodes in hidden layers\n",
    "\n",
    "- *kernel_initializer* param corresponds to the way we are initializing our weights:\n",
    "\n",
    "As we saw during the course, weights must be initialized as small numbers close to zero. The random uniform function allows us to initialize our weights in this way.\n",
    "\n",
    "- *activation* param corresponds to the activation function for hidden layers:\n",
    "\n",
    "We choose 'relu' for rectifier function.\n",
    "\n",
    "- *input_shape* is the number of nodes in the input layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, letâ€™s create our second hidden layer, which will be the same as the first one. Here, we donâ€™t have to specify the input shape because it can deduce this with the previous hidden layer. For the first one, there was no hidden layer yet created so we had to specify it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to add our output layer. We have to change the activation function because we said that the activation function of our output layer will be the sigmoid function. We also want a single binary output so the units (output dimension) will be set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to apply stochastic gradient method on the whole neural network by compiling our model. Indeed, we have built our ANN but the weigths are still initialized, so now we need to find the best weights that will make our NN the most powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilling the ANN\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *optimizer* is the algorithm which will be used to find the optimal set of weights in the ANN, here we will use (as we seen in the course) the stochastic gradient descent method. There are several types of this method, a very efficient one is called 'adam', we will use this one.\n",
    "\n",
    "- *loss* is the loss function to be optimized within the SGD method. We will use here a log loss because the activation function of our output layer is the sigmoid function (this is the same as for logistic regression model).\n",
    "**tip**: for more than two categories, the log loss function is called \"categorical_crossentropy\"\n",
    "\n",
    "- *metrics* is the criterion that we use to evaluate our model. Here we are going to choose 'accuracy'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to train our NN with the fit method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 2s 227us/step - loss: 0.4929 - acc: 0.7952\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.4190 - acc: 0.7960\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 0.4102 - acc: 0.7960\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.4016 - acc: 0.8229\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 0.3941 - acc: 0.8295\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 0.3872 - acc: 0.8309\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 0.3816 - acc: 0.8365\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 0.3775 - acc: 0.8431\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.3734 - acc: 0.8447\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 0.3706 - acc: 0.8474\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 0.3690 - acc: 0.8501\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 0.3666 - acc: 0.8495\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 0.3651 - acc: 0.8507\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 0.3629 - acc: 0.8509\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 0.3617 - acc: 0.8491\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 0.3609 - acc: 0.8509\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 0.3589 - acc: 0.8545\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.3589 - acc: 0.8529\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 0.3581 - acc: 0.8530\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 0.3573 - acc: 0.8535\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.3569 - acc: 0.8505\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 0.3558 - acc: 0.8547\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 152us/step - loss: 0.3555 - acc: 0.8552\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s 151us/step - loss: 0.3549 - acc: 0.8540\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 154us/step - loss: 0.3548 - acc: 0.8562\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 0.3544 - acc: 0.8542\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3545 - acc: 0.8549\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 0.3540 - acc: 0.8546\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 0.3535 - acc: 0.8587\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.3534 - acc: 0.8560\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.3533 - acc: 0.8575\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 0.3526 - acc: 0.8555\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 0.3519 - acc: 0.8559\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 0.3522 - acc: 0.8587\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 0.3515 - acc: 0.8557\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3519 - acc: 0.8564\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 0.3517 - acc: 0.8555\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 0.3506 - acc: 0.8559\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 0.3513 - acc: 0.8576\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 0.3513 - acc: 0.8577\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 0.3504 - acc: 0.8596\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.3507 - acc: 0.8576\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3507 - acc: 0.8564\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 0.3509 - acc: 0.8565\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.3505 - acc: 0.8577\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 0.3506 - acc: 0.8572\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 2s 188us/step - loss: 0.3507 - acc: 0.8594\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.3505 - acc: 0.8576\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 0.3506 - acc: 0.8571\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 0.3488 - acc: 0.8557\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 0.3506 - acc: 0.8581\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 0.3497 - acc: 0.8575\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 0.3498 - acc: 0.8577\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.3495 - acc: 0.8587\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 0.3499 - acc: 0.8574\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 0.3492 - acc: 0.8555\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3495 - acc: 0.8589\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.3486 - acc: 0.8582\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 0.3499 - acc: 0.8565\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 0.3487 - acc: 0.8596\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3492 - acc: 0.8562\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 2s 274us/step - loss: 0.3487 - acc: 0.8574\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 0.3482 - acc: 0.8569\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 0.3481 - acc: 0.8601\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.3476 - acc: 0.8616\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 0.3480 - acc: 0.8567\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 0.3477 - acc: 0.8572\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 0.3472 - acc: 0.8582\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 0.3479 - acc: 0.8592\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 0.3480 - acc: 0.8584\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.3470 - acc: 0.8586\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 0.3473 - acc: 0.8594\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 0.3466 - acc: 0.8584\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.3475 - acc: 0.8582\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 0.3471 - acc: 0.8619\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 0.3464 - acc: 0.8582\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 0.3470 - acc: 0.8590\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 0.3462 - acc: 0.8602\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 0.3467 - acc: 0.8604\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 0.3458 - acc: 0.8594\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 0.3459 - acc: 0.8604\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 0.3459 - acc: 0.8599\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 2s 296us/step - loss: 0.3456 - acc: 0.8600\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.3451 - acc: 0.8602\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 2s 268us/step - loss: 0.3448 - acc: 0.8616\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 2s 233us/step - loss: 0.3454 - acc: 0.8602\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 2s 263us/step - loss: 0.3452 - acc: 0.8610\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 2s 250us/step - loss: 0.3451 - acc: 0.8595\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 0.3455 - acc: 0.8606\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 0.3449 - acc: 0.8610\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 2s 240us/step - loss: 0.3447 - acc: 0.8607\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.3453 - acc: 0.8625\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 0.3451 - acc: 0.8611\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 0.3454 - acc: 0.8589\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 0.3447 - acc: 0.8597\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 0.3453 - acc: 0.8611\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 0.3444 - acc: 0.8610\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 0.3446 - acc: 0.8616\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 1s 187us/step - loss: 0.3449 - acc: 0.8629\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.3439 - acc: 0.8625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x102e54ef0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, batch_size=10, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *batch_size* is the number of observations after which you want to update the weights.\n",
    "\n",
    "- *epochs*, an epoch is an iteration over the entire x and y data provided. We need to repeat step one to six several time (number of epoch) on our dataset to train the model.\n",
    "\n",
    "For this two arguments, there is no rule and the experimentation is the best method to find out the best number to input.\n",
    "\n",
    "After running this cell, we can see how stochastic gradient descent if performing on our dataset and how accuracy is improving as the number of epochs realised increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
