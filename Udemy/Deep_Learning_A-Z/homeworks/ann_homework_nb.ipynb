{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning A-Z : Building an ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is my response to the first homework of the course called *Deep Learning A-Z™: Hands-On Artificial Neural Networks* accessible here : https://www.udemy.com/deeplearning/\n",
    "\n",
    "In this notebook, we are going to build an ANN using keras and by following instructions given on the course. This neurals network will predict, for a customer of a bank, if this customer is going to leave the bank or not. We are going to train our ann with a dataset containing data about approximately 10000 clients, which also includes a response column in which we can see whether the client stayed or not in the bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_train = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))), 'ressources/Artificial_Neural_Networks/Churn_Modelling.csv')\n",
    "dataset = pd.read_csv(path_train)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      "RowNumber          10000 non-null int64\n",
      "CustomerId         10000 non-null int64\n",
      "Surname            10000 non-null object\n",
      "CreditScore        10000 non-null int64\n",
      "Geography          10000 non-null object\n",
      "Gender             10000 non-null object\n",
      "Age                10000 non-null int64\n",
      "Tenure             10000 non-null int64\n",
      "Balance            10000 non-null float64\n",
      "NumOfProducts      10000 non-null int64\n",
      "HasCrCard          10000 non-null int64\n",
      "IsActiveMember     10000 non-null int64\n",
      "EstimatedSalary    10000 non-null float64\n",
      "Exited             10000 non-null int64\n",
      "dtypes: float64(2), int64(9), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Exited\" is our response column, its the 14th column, we are going to need this information to modify the template given by the course to make it works for this case. The three first columns have no impact on the response so we will not include it in out training table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the course, we are going to preprocess our data using the template given in the course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([608, 'Spain', 'Female', 41, 1, 83807.86, 1, 0, 1, 112542.58],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:, 3:13].values # we modify indexes according to what we saw with the info() method of the dataset\n",
    "y = dataset.iloc[:, 13].values # idem\n",
    "X[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to encode our categorical variables. We are going to do it using the template of the course. Here we have two categorical columns (Geography and Gender) so we have to create to encoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "labelencoder_X_geo = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_geo.fit_transform(X[:, 1])\n",
    "labelencoder_X_gender = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_gender.fit_transform(X[:, 2])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 0.0000000e+00, 6.1900000e+02, 0.0000000e+00,\n",
       "       4.2000000e+01, 2.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 1.0134888e+05])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Feature Scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data are now preprocessed ! We can start building our model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Let's build our ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing our ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, here are all the steps we must follow for training an ANN with stochastic gradient descent method. Dense function will be used for step 1. From step 2, we know that each features is attributed to one node, so we have to create 11 input nodes in our input layer. \n",
    "We also have to choose an activation function (step 3) as we saw in the course, we will use the best one for our hidden layers (based on experiment) : the rectifier function. The sigmoid function is a very good option for our output layer because it will gives us probabilities for each classes.\n",
    "After that, we will use learning rate to choose how weights are updated and we will also think about how many epochs we are going to do. Let's go !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/steps.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the first hidden layer of our ANN\n",
    "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_shape = (11,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *units* param corresponds to the number of node of the layer:\n",
    "\n",
    "**tip :** choose the number of nodes in the hidden layers as the average of the number of nodes in the input layer and the number of nodes in the output layer\n",
    "Here we have 11 nodes in input layer and 1 node in output layer (because binary output) so we choose 6 nodes in hidden layers\n",
    "\n",
    "- *kernel_initializer* param corresponds to the way we are initializing our weights:\n",
    "\n",
    "As we saw during the course, weights must be initialized as small numbers close to zero. The random uniform function allows us to initialize our weights in this way.\n",
    "\n",
    "- *activation* param corresponds to the activation function for hidden layers:\n",
    "\n",
    "We choose 'relu' for rectifier function.\n",
    "\n",
    "- *input_shape* is the number of nodes in the input layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s create our second hidden layer, which will be the same as the first one. Here, we don’t have to specify the input shape because it can deduce this with the previous hidden layer. For the first one, there was no hidden layer yet created so we had to specify it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to add our output layer. We have to change the activation function because we said that the activation function of our output layer will be the sigmoid function. We also want a single binary output so the units (output dimension) will be set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to apply stochastic gradient method on the whole neural network by compiling our model. Indeed, we have built our ANN but the weigths are still initialized, so now we need to find the best weights that will make our NN the most powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilling the ANN\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *optimizer* is the algorithm which will be used to find the optimal set of weights in the ANN, here we will use (as we seen in the course) the stochastic gradient descent method. There are several types of this method, a very efficient one is called 'adam', we will use this one.\n",
    "\n",
    "- *loss* is the loss function to be optimized within the SGD method. We will use here a log loss because the activation function of our output layer is the sigmoid function (this is the same as for logistic regression model).\n",
    "**tip**: for more than two categories, the log loss function is called \"categorical_crossentropy\"\n",
    "\n",
    "- *metrics* is the criterion that we use to evaluate our model. Here we are going to choose 'accuracy'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to train our NN with the fit method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 2s 227us/step - loss: 0.4929 - acc: 0.7952\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 0.4190 - acc: 0.7960\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 0.4102 - acc: 0.7960\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.4016 - acc: 0.8229\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 0.3941 - acc: 0.8295\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 0.3872 - acc: 0.8309\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 0.3816 - acc: 0.8365\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 0.3775 - acc: 0.8431\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.3734 - acc: 0.8447\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 0.3706 - acc: 0.8474\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 0.3690 - acc: 0.8501\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 0.3666 - acc: 0.8495\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 0.3651 - acc: 0.8507\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 0.3629 - acc: 0.8509\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 0.3617 - acc: 0.8491\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 0.3609 - acc: 0.8509\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 0.3589 - acc: 0.8545\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.3589 - acc: 0.8529\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 0.3581 - acc: 0.8530\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 0.3573 - acc: 0.8535\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.3569 - acc: 0.8505\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 0.3558 - acc: 0.8547\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 152us/step - loss: 0.3555 - acc: 0.8552\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s 151us/step - loss: 0.3549 - acc: 0.8540\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 154us/step - loss: 0.3548 - acc: 0.8562\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 0.3544 - acc: 0.8542\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3545 - acc: 0.8549\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 0.3540 - acc: 0.8546\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 0.3535 - acc: 0.8587\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.3534 - acc: 0.8560\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.3533 - acc: 0.8575\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 0.3526 - acc: 0.8555\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 0.3519 - acc: 0.8559\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 0.3522 - acc: 0.8587\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 0.3515 - acc: 0.8557\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 0.3519 - acc: 0.8564\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 0.3517 - acc: 0.8555\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 0.3506 - acc: 0.8559\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 0.3513 - acc: 0.8576\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 0.3513 - acc: 0.8577\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 0.3504 - acc: 0.8596\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.3507 - acc: 0.8576\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.3507 - acc: 0.8564\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 0.3509 - acc: 0.8565\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.3505 - acc: 0.8577\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 0.3506 - acc: 0.8572\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 2s 188us/step - loss: 0.3507 - acc: 0.8594\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.3505 - acc: 0.8576\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 0.3506 - acc: 0.8571\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 0.3488 - acc: 0.8557\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 0.3506 - acc: 0.8581\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 0.3497 - acc: 0.8575\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 0.3498 - acc: 0.8577\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.3495 - acc: 0.8587\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 0.3499 - acc: 0.8574\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 0.3492 - acc: 0.8555\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.3495 - acc: 0.8589\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.3486 - acc: 0.8582\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 0.3499 - acc: 0.8565\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 0.3487 - acc: 0.8596\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 0.3492 - acc: 0.8562\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 2s 274us/step - loss: 0.3487 - acc: 0.8574\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 0.3482 - acc: 0.8569\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 0.3481 - acc: 0.8601\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.3476 - acc: 0.8616\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 0.3480 - acc: 0.8567\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 0.3477 - acc: 0.8572\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 0.3472 - acc: 0.8582\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 0.3479 - acc: 0.8592\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 0.3480 - acc: 0.8584\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.3470 - acc: 0.8586\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 0.3473 - acc: 0.8594\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 0.3466 - acc: 0.8584\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.3475 - acc: 0.8582\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 0.3471 - acc: 0.8619\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 0.3464 - acc: 0.8582\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 0.3470 - acc: 0.8590\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 0.3462 - acc: 0.8602\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 0.3467 - acc: 0.8604\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 0.3458 - acc: 0.8594\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 0.3459 - acc: 0.8604\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 0.3459 - acc: 0.8599\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 2s 296us/step - loss: 0.3456 - acc: 0.8600\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.3451 - acc: 0.8602\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 2s 268us/step - loss: 0.3448 - acc: 0.8616\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 2s 233us/step - loss: 0.3454 - acc: 0.8602\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 2s 263us/step - loss: 0.3452 - acc: 0.8610\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 2s 250us/step - loss: 0.3451 - acc: 0.8595\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 0.3455 - acc: 0.8606\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 0.3449 - acc: 0.8610\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 2s 240us/step - loss: 0.3447 - acc: 0.8607\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.3453 - acc: 0.8625\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 0.3451 - acc: 0.8611\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 0.3454 - acc: 0.8589\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 0.3447 - acc: 0.8597\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 0.3453 - acc: 0.8611\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 0.3444 - acc: 0.8610\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 0.3446 - acc: 0.8616\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 1s 187us/step - loss: 0.3449 - acc: 0.8629\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.3439 - acc: 0.8625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x102e54ef0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the ANN\n",
    "classifier.fit(X_train, y_train, batch_size=10, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *batch_size* is the number of observations after which you want to update the weights.\n",
    "\n",
    "- *epochs*, an epoch is an iteration over the entire x and y data provided. We need to repeat step one to six several time (number of epoch) on our dataset to train the model.\n",
    "\n",
    "For this two arguments, there is no rule and the experimentation is the best method to find out the best number to input.\n",
    "\n",
    "After running this cell, we can see how stochastic gradient descent if performing on our dataset and how accuracy is improving as the number of epochs realised increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Making predictions and evaluating model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can make our prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1539,   56],\n",
       "       [ 212,  193]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5) # convert probabilities to binary output\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a better plot of the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square=True, cmap='Blues_r')\n",
    "plt.ylabel('Actual exited label')\n",
    "plt.xlabel('Predicted exited label')\n",
    "fig.xaxis.set_ticklabels(['Stay', 'Leave'])\n",
    "fig.yaxis.set_ticklabels(['Stay', 'Leave']);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1539 + 193 = 1732 correct predictions and 268 incorrect predictions. We can calculate the accuracy on test dataset (so this is the accuracy on new observations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.866"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = 1732/2000\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use our ANN model to predict if the customer with the following informations will leave the bank: \n",
    "\n",
    "- Geography: France\n",
    "- Credit Score: 600\n",
    "- Gender: Male\n",
    "- Age: 40 years old\n",
    "- Tenure: 3 years\n",
    "- Balance: 60000 dollars\n",
    "- Number of Products: 2\n",
    "- Does this customer have a credit card ? Yes\n",
    "- Is this customer an Active Member: Yes\n",
    "- Estimated Salary: 50000 dollars\n",
    "\n",
    "So should we say goodbye to that customer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by creating a dataframe containing the observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>600</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId  Surname  CreditScore Geography Gender  Age  Tenure  \\\n",
       "0          0           0        0          600    France   Male   40       3   \n",
       "\n",
       "   Balance  NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  Exited  \n",
       "0  60000.0              2          1               1          50000.0       0  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = dataset.columns.tolist()\n",
    "data = [0,0,0, 600, 'France', 'Male', 40, 3, 60000.00, 2, 1, 1, 50000.00, 0]\n",
    "table = {columns[n]:[data[n]] for n in range(14)}\n",
    "homework_customer = pd.DataFrame(table)\n",
    "homework_customer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add our observation to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([600, 'France', 'Male', 40, 3, 60000.0, 2, 1, 1, 50000.0],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_homework = dataset.copy()\n",
    "dataset_homework = dataset_homework.append(homework_customer)\n",
    "X_homework = dataset_homework.iloc[:, 3:13].values\n",
    "y_homework = dataset_homework.iloc[:, 13].values # idem\n",
    "X_homework[10000] # here is our observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can standardize the data like above (encode cathegorical features and feature scaling) and make the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04758769]], dtype=float32)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_homework[:, 1] = labelencoder_X_geo.fit_transform(X_homework[:, 1])\n",
    "X_homework[:, 2] = labelencoder_X_gender.fit_transform(X_homework[:, 2])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X_homework = onehotencoder.fit_transform(X_homework).toarray()\n",
    "X_homework = X_homework[:, 1:]\n",
    "X_homework = sc.transform(X_homework)\n",
    "homework_observation = X_homework[10000:]\n",
    "homework_prediction = classifier.predict(homework_observation)\n",
    "homework_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predict a probability of 0.047 for this customer to leave the bank, so this customer is going to **stay** in the bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was an easiest way to do it. The fact is that the StandarScaler object will apply the same scale than previously when it was fitted. Futhermore, We could just look at the dummies variables of a similar column (France and Male obs) to create an array, there was no need to reapply encoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04758769]], dtype=float32)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prediction = classifier.predict(sc.transform(np.array([[0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])))\n",
    "new_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same result than found above !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluating the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s important to evaluate the model to ensure that accuracy is not very different from one training to another, in other words we want to ensure that our model has a low variance. To evaluate our model, we will use the K-fold cross validation method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ann():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_shape = (11,)))\n",
    "    classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))\n",
    "    classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "classifier = KerasClassifier(build_fn=build_ann, batch_size=10, epochs=100, verbose=0)\n",
    "accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=10, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85999999, 0.8325    , 0.88124999, 0.84874999, 0.87124999,\n",
       "       0.83375   , 0.8325    , 0.845     , 0.8175    , 0.86374999])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the true accuracy of our model and its variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.8486249951273204 \n",
      "Variance: 0.01912499840888716\n"
     ]
    }
   ],
   "source": [
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()\n",
    "print('Mean accuracy:', mean, '\\nVariance:', variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of 86% obtained in our first training of the model was not relevant ! But now, let's try to reach it by tuning our model !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Dropout regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some dropout regularization to our two hidden layers to reduce overfitting.\n",
    "At each iteration of the training some neurons of your artificial neural network are randomly disabled to prevent them from being too dependent on each other when they learn the correlations and therefore, by overwriting these neurons the artificial neural network learns several independent correlations in the data because each time there is not the same configuration of the neurons. And the fact that we get these independent correlations of the data, thanks to the fact that the neurons work more independently, that prevents the neurons from learning too much and therefore that prevents overfitting.\n",
    "\n",
    "- *p* is the fraction of the neurons we want to drop. In general, begin with 10% and increase this value (by 10% steps) if there is still overfitting. Don't try to go over 50% to avoid underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing our ANN\n",
    "classifier = Sequential()\n",
    "# Adding the input layer and the first hidden layer of our ANN with dropout\n",
    "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_shape = (11,)))\n",
    "classifier.add(Dropout(p=0.1)) \n",
    "# Adding the second hidden layer with dropout\n",
    "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))\n",
    "classifier.add(Dropout(p=0.1))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "# Compilling the ANN\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model tuning (homework : get over 86% accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 9s 1ms/step - loss: 0.5805 - acc: 0.7959\n",
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 2s 342us/step - loss: 0.4458 - acc: 0.7980\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 2s 311us/step - loss: 0.4412 - acc: 0.7980\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 2s 299us/step - loss: 0.4347 - acc: 0.7980\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 2s 305us/step - loss: 0.4321 - acc: 0.7980\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 2s 301us/step - loss: 0.4314 - acc: 0.7980\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 2s 322us/step - loss: 0.4304 - acc: 0.7980\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 2s 308us/step - loss: 0.4315 - acc: 0.7980\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 2s 308us/step - loss: 0.4269 - acc: 0.7980\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 2s 311us/step - loss: 0.4282 - acc: 0.7980\n",
      "1600/1600 [==============================] - 3s 2ms/step\n",
      "6400/6400 [==============================] - 2s 243us/step\n",
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 9s 1ms/step - loss: 0.5666 - acc: 0.7966\n",
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 2s 312us/step - loss: 0.4390 - acc: 0.7972\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 2s 328us/step - loss: 0.4335 - acc: 0.7972\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 2s 321us/step - loss: 0.4309 - acc: 0.7972\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 2s 315us/step - loss: 0.4289 - acc: 0.7972\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 2s 320us/step - loss: 0.4255 - acc: 0.7972\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 2s 310us/step - loss: 0.4266 - acc: 0.7972\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 2s 330us/step - loss: 0.4263 - acc: 0.7972\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 2s 314us/step - loss: 0.4228 - acc: 0.7972\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 2s 320us/step - loss: 0.4228 - acc: 0.8105\n",
      "1600/1600 [==============================] - 3s 2ms/step\n",
      "6400/6400 [==============================] - 2s 239us/step\n",
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 9s 1ms/step - loss: 0.5615 - acc: 0.7909\n",
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 2s 348us/step - loss: 0.4483 - acc: 0.7917\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 2s 325us/step - loss: 0.4428 - acc: 0.7917\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 2s 359us/step - loss: 0.4405 - acc: 0.7917\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 2s 361us/step - loss: 0.4384 - acc: 0.7917\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 2s 325us/step - loss: 0.4387 - acc: 0.7917\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 2s 316us/step - loss: 0.4364 - acc: 0.7917\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 2s 330us/step - loss: 0.4354 - acc: 0.7917\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 2s 335us/step - loss: 0.4328 - acc: 0.7917\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 2s 318us/step - loss: 0.4336 - acc: 0.7917\n",
      "1600/1600 [==============================] - 3s 2ms/step\n",
      "6400/6400 [==============================] - 2s 299us/step\n",
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 9s 1ms/step - loss: 0.5807 - acc: 0.7963\n",
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 2s 333us/step - loss: 0.4451 - acc: 0.7973\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 2s 338us/step - loss: 0.4285 - acc: 0.8033\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 2s 368us/step - loss: 0.4171 - acc: 0.8102\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 2s 269us/step - loss: 0.4094 - acc: 0.8144\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 2s 273us/step - loss: 0.4018 - acc: 0.8158\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 3s 403us/step - loss: 0.3908 - acc: 0.8198\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 2s 375us/step - loss: 0.3962 - acc: 0.8308\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 2s 365us/step - loss: 0.3869 - acc: 0.8409\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 2s 333us/step - loss: 0.3836 - acc: 0.8425\n",
      "1600/1600 [==============================] - 4s 3ms/step\n",
      "6400/6400 [==============================] - 2s 306us/step\n",
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 10s 2ms/step - loss: 0.5509 - acc: 0.7958\n",
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 2s 317us/step - loss: 0.4395 - acc: 0.7958\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 3s 442us/step - loss: 0.4337 - acc: 0.7958\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 2s 301us/step - loss: 0.4335 - acc: 0.7958\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 2s 277us/step - loss: 0.4296 - acc: 0.7958\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 2s 327us/step - loss: 0.4282 - acc: 0.7958\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 2s 294us/step - loss: 0.4274 - acc: 0.7958\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 2s 355us/step - loss: 0.4262 - acc: 0.7958\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 2s 335us/step - loss: 0.4254 - acc: 0.8003\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 2s 307us/step - loss: 0.4235 - acc: 0.8206\n",
      "1600/1600 [==============================] - 4s 2ms/step\n",
      "6400/6400 [==============================] - 1s 212us/step\n",
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 11s 2ms/step - loss: 0.6077 - acc: 0.7956\n",
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 3s 400us/step - loss: 0.4578 - acc: 0.7980\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 2s 347us/step - loss: 0.4352 - acc: 0.7980\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 2s 281us/step - loss: 0.4296 - acc: 0.7989\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 2s 303us/step - loss: 0.4254 - acc: 0.8159\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 2s 285us/step - loss: 0.4203 - acc: 0.8195\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 2s 319us/step - loss: 0.4174 - acc: 0.8259\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 2s 291us/step - loss: 0.4154 - acc: 0.8234\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 2s 272us/step - loss: 0.4172 - acc: 0.8220\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 2s 279us/step - loss: 0.4117 - acc: 0.8222\n",
      "1600/1600 [==============================] - 4s 2ms/step\n",
      "6400/6400 [==============================] - 1s 232us/step\n",
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 10s 2ms/step - loss: 0.6155 - acc: 0.7959\n",
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 2s 278us/step - loss: 0.4694 - acc: 0.7972\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 2s 290us/step - loss: 0.4475 - acc: 0.7972\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 2s 280us/step - loss: 0.4384 - acc: 0.7972\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 2s 262us/step - loss: 0.4382 - acc: 0.7972\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 2s 259us/step - loss: 0.4359 - acc: 0.7972\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 2s 260us/step - loss: 0.4334 - acc: 0.7972\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 2s 268us/step - loss: 0.4320 - acc: 0.7972\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 2s 262us/step - loss: 0.4297 - acc: 0.7972\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 2s 263us/step - loss: 0.4317 - acc: 0.7972\n",
      "1600/1600 [==============================] - 3s 2ms/step\n",
      "6400/6400 [==============================] - 1s 202us/step\n",
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 10s 2ms/step - loss: 0.6154 - acc: 0.7898\n",
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 2s 269us/step - loss: 0.4673 - acc: 0.7917\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 2s 267us/step - loss: 0.4441 - acc: 0.7917\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 2s 297us/step - loss: 0.4368 - acc: 0.7917\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 2s 269us/step - loss: 0.4355 - acc: 0.7917\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 2s 268us/step - loss: 0.4331 - acc: 0.7917\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 2s 288us/step - loss: 0.4307 - acc: 0.7917\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 2s 270us/step - loss: 0.4267 - acc: 0.7917\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 2s 294us/step - loss: 0.4244 - acc: 0.7917\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 2s 285us/step - loss: 0.4235 - acc: 0.7917\n",
      "1600/1600 [==============================] - 3s 2ms/step\n",
      "6400/6400 [==============================] - 1s 207us/step\n",
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 10s 2ms/step - loss: 0.5884 - acc: 0.7963\n",
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 2s 285us/step - loss: 0.4479 - acc: 0.7973\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 2s 287us/step - loss: 0.4362 - acc: 0.7973\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 2s 289us/step - loss: 0.4342 - acc: 0.7973\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 2s 279us/step - loss: 0.4328 - acc: 0.7973\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 2s 280us/step - loss: 0.4308 - acc: 0.7973\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 2s 266us/step - loss: 0.4291 - acc: 0.7973\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 2s 328us/step - loss: 0.4258 - acc: 0.7973\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 3s 447us/step - loss: 0.4268 - acc: 0.7973\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 2s 376us/step - loss: 0.4232 - acc: 0.7973\n",
      "1600/1600 [==============================] - 4s 3ms/step\n",
      "6400/6400 [==============================] - 2s 269us/step\n",
      "Epoch 1/10\n",
      "6400/6400 [==============================] - 13s 2ms/step - loss: 0.6113 - acc: 0.7945\n",
      "Epoch 2/10\n",
      "6400/6400 [==============================] - 2s 299us/step - loss: 0.4675 - acc: 0.7958\n",
      "Epoch 3/10\n",
      "6400/6400 [==============================] - 2s 278us/step - loss: 0.4417 - acc: 0.7958\n",
      "Epoch 4/10\n",
      "6400/6400 [==============================] - 2s 273us/step - loss: 0.4358 - acc: 0.7958\n",
      "Epoch 5/10\n",
      "6400/6400 [==============================] - 2s 270us/step - loss: 0.4319 - acc: 0.7958\n",
      "Epoch 6/10\n",
      "6400/6400 [==============================] - 2s 270us/step - loss: 0.4343 - acc: 0.7958\n",
      "Epoch 7/10\n",
      "6400/6400 [==============================] - 2s 273us/step - loss: 0.4299 - acc: 0.7958\n",
      "Epoch 8/10\n",
      "6400/6400 [==============================] - 2s 347us/step - loss: 0.4260 - acc: 0.7958\n",
      "Epoch 9/10\n",
      "6400/6400 [==============================] - 2s 306us/step - loss: 0.4236 - acc: 0.7958\n",
      "Epoch 10/10\n",
      "6400/6400 [==============================] - 2s 306us/step - loss: 0.4194 - acc: 0.7958\n",
      "1600/1600 [==============================] - 4s 2ms/step\n",
      "6400/6400 [==============================] - 1s 229us/step\n",
      "Epoch 1/20\n",
      "6400/6400 [==============================] - 10s 2ms/step - loss: 0.6116 - acc: 0.7958\n",
      "Epoch 2/20\n",
      "6400/6400 [==============================] - 2s 291us/step - loss: 0.4446 - acc: 0.7989\n",
      "Epoch 3/20\n",
      "6400/6400 [==============================] - 2s 281us/step - loss: 0.4280 - acc: 0.8177\n",
      "Epoch 4/20\n",
      "6400/6400 [==============================] - 2s 282us/step - loss: 0.4190 - acc: 0.8225\n",
      "Epoch 5/20\n",
      "6400/6400 [==============================] - 2s 274us/step - loss: 0.4197 - acc: 0.8166\n",
      "Epoch 6/20\n",
      "6400/6400 [==============================] - 2s 281us/step - loss: 0.4163 - acc: 0.8187\n",
      "Epoch 7/20\n",
      "6400/6400 [==============================] - 2s 287us/step - loss: 0.4163 - acc: 0.8161\n",
      "Epoch 8/20\n",
      "6400/6400 [==============================] - 2s 284us/step - loss: 0.4140 - acc: 0.8175\n",
      "Epoch 9/20\n",
      "6400/6400 [==============================] - 2s 302us/step - loss: 0.4120 - acc: 0.8203\n",
      "Epoch 10/20\n",
      "6400/6400 [==============================] - 2s 287us/step - loss: 0.4121 - acc: 0.8200\n",
      "Epoch 11/20\n",
      "6400/6400 [==============================] - 2s 284us/step - loss: 0.4118 - acc: 0.8184\n",
      "Epoch 12/20\n",
      "6400/6400 [==============================] - 2s 283us/step - loss: 0.4076 - acc: 0.8183\n",
      "Epoch 13/20\n",
      "6400/6400 [==============================] - 2s 332us/step - loss: 0.4091 - acc: 0.8213\n",
      "Epoch 14/20\n",
      "6400/6400 [==============================] - 3s 412us/step - loss: 0.4122 - acc: 0.8216\n",
      "Epoch 15/20\n",
      "6400/6400 [==============================] - 2s 318us/step - loss: 0.4079 - acc: 0.8175\n",
      "Epoch 16/20\n",
      "6400/6400 [==============================] - 2s 333us/step - loss: 0.4101 - acc: 0.8222\n",
      "Epoch 17/20\n",
      "6400/6400 [==============================] - 2s 342us/step - loss: 0.4058 - acc: 0.8198\n",
      "Epoch 18/20\n",
      "6400/6400 [==============================] - 2s 315us/step - loss: 0.4080 - acc: 0.8178\n",
      "Epoch 19/20\n",
      "6400/6400 [==============================] - 2s 304us/step - loss: 0.4056 - acc: 0.8233\n",
      "Epoch 20/20\n",
      "6400/6400 [==============================] - 2s 279us/step - loss: 0.4063 - acc: 0.8217\n",
      "1600/1600 [==============================] - 4s 2ms/step\n",
      "6400/6400 [==============================] - 1s 201us/step\n",
      "Epoch 1/20\n",
      "6400/6400 [==============================] - 10s 2ms/step - loss: 0.5956 - acc: 0.7962\n",
      "Epoch 2/20\n",
      "6400/6400 [==============================] - 2s 295us/step - loss: 0.4470 - acc: 0.7972\n",
      "Epoch 3/20\n",
      "6400/6400 [==============================] - 2s 294us/step - loss: 0.4419 - acc: 0.7972\n",
      "Epoch 4/20\n",
      "6400/6400 [==============================] - 2s 314us/step - loss: 0.4391 - acc: 0.7972\n",
      "Epoch 5/20\n",
      "6400/6400 [==============================] - 2s 299us/step - loss: 0.4374 - acc: 0.7972\n",
      "Epoch 6/20\n",
      "6400/6400 [==============================] - 2s 310us/step - loss: 0.4354 - acc: 0.7972\n",
      "Epoch 7/20\n",
      "6400/6400 [==============================] - 2s 299us/step - loss: 0.4351 - acc: 0.7972\n",
      "Epoch 8/20\n",
      "6400/6400 [==============================] - 2s 292us/step - loss: 0.4319 - acc: 0.7972\n",
      "Epoch 9/20\n",
      "6400/6400 [==============================] - 2s 295us/step - loss: 0.4350 - acc: 0.7972\n",
      "Epoch 10/20\n",
      "6400/6400 [==============================] - 2s 298us/step - loss: 0.4305 - acc: 0.7972\n",
      "Epoch 11/20\n",
      "6400/6400 [==============================] - 2s 295us/step - loss: 0.4324 - acc: 0.7972\n",
      "Epoch 12/20\n",
      "6400/6400 [==============================] - 2s 332us/step - loss: 0.4317 - acc: 0.7972\n",
      "Epoch 13/20\n",
      "6400/6400 [==============================] - 2s 297us/step - loss: 0.4306 - acc: 0.7972\n",
      "Epoch 14/20\n",
      "6400/6400 [==============================] - 2s 291us/step - loss: 0.4322 - acc: 0.7972\n",
      "Epoch 15/20\n",
      "6400/6400 [==============================] - 2s 303us/step - loss: 0.4290 - acc: 0.7972\n",
      "Epoch 16/20\n",
      "6400/6400 [==============================] - 2s 293us/step - loss: 0.4303 - acc: 0.7972\n",
      "Epoch 17/20\n",
      "6400/6400 [==============================] - 2s 305us/step - loss: 0.4313 - acc: 0.7972\n",
      "Epoch 18/20\n",
      "6400/6400 [==============================] - 2s 317us/step - loss: 0.4306 - acc: 0.7972\n",
      "Epoch 19/20\n",
      "6400/6400 [==============================] - 2s 293us/step - loss: 0.4284 - acc: 0.7972\n",
      "Epoch 20/20\n",
      "6400/6400 [==============================] - 2s 293us/step - loss: 0.4295 - acc: 0.7972\n"
     ]
    }
   ],
   "source": [
    "def build_ann(optimizer):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_shape = (11,)))\n",
    "    classifier.add(Dropout(p=0.1)) \n",
    "    classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))\n",
    "    classifier.add(Dropout(p=0.1)) \n",
    "    classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    classifier.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "classifier = KerasClassifier(build_fn=build_ann, verbose=1)\n",
    "parameters = {\n",
    "    'batch_size' : [25, 38],\n",
    "    'epochs' : [100, 510],\n",
    "    'optimizer' : ['adam', 'rmsprop']\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=classifier, param_grid=parameters, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "print('Best accuracy:', best_accuracy, '\\nBest parameters:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
