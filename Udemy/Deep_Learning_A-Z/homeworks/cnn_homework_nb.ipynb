{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning A-Z : Building a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is my response to the second homework of the course called *Deep Learning A-Z™: Hands-On Artificial Neural Networks* accessible here : https://www.udemy.com/deeplearning/\n",
    "\n",
    "In this notebook, we are going to build an CNN using keras and by following instructions given on the course. This neurals network will predict, for an image, if this a cat or a dog image. We are going to train our ann with a lot of cat and dog images and after that, we are going to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no data preprocessing with images because independant variables are our pixels and data are already divided into train and test set in the structure of our projet. However, we will need to standardize our data before fiting our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing our CNN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we seen it in the course, we are going to ad a layer for each step for processing images before passing it to a classic ann (full connection):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/steps_cnn.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3,3), input_shape=(64, 64, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *filters* is the number of feature detector we will use (common practice = start with 32). C\n",
    "- *kernel_size* is the size of the feature detectors.\n",
    "- *input_shape* used to fix the size of our images, which have by default differents sizes. Third number corresponds to number of channels (color or B&W). 6' is enough here because we work on a CPU, but an be increase for a better accuracy on GPU.\n",
    "- *activation* is used, as seen in the course, to remove non-linearity on our ouput convolutioned images. We will use relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Max Pooling\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *pool_size* corresponds to the size of the max pooling subtable used to reduce size of feature map. In general, 2x2 matrix is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Full connection, add hidden layer (fully connected layer)\n",
    "classifier.add(Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Full connection - add output layer\n",
    "classifier.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use an images augmentation process using keras to reduce overfitting which can appears because of our few number of images (only 8000 for training). Image augmentation allows us to enrich our dataset without adding new images. It will, for each images, create a batch of images with random transformation on it (rotating, flipping, shifting …) so it will provide a lot more material to train our CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))), \n",
    "                          'ressources/Convolutional_Neural_Networks/dataset/training_set')\n",
    "path_test = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))), \n",
    "                          'ressources/Convolutional_Neural_Networks/dataset/test_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 3360s 420ms/step - loss: 0.2449 - acc: 0.8966 - val_loss: 0.9785 - val_acc: 0.7531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2dd414e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for image augmentation found here : https://keras.io/preprocessing/image/\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True) # Apply image augmentation on train images\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) # Do not apply transformation on test images\n",
    "\n",
    "# target size is the same as choosen in our cnn architecture\n",
    "training_set = train_datagen.flow_from_directory(path_train, target_size=(64, 64),batch_size=32, \n",
    "                                                 class_mode='binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(path_test, target_size=(64, 64), batch_size=32, \n",
    "                                            class_mode='binary')\n",
    "\n",
    "# fit model\n",
    "classifier.fit_generator(training_set, \n",
    "                         steps_per_epoch=8000, # number of images in training_set\n",
    "                         epochs=1, # One epoch because my computer is too slow (need to be increased)\n",
    "                         validation_data=test_set, \n",
    "                         validation_steps=2000) # number of images in training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is not as good as we would expect on test set ... How can we increase it ? By building a deeper network ! We have the choice between adding another convolutionnal layer or adding another fully connected layer. In fact, adding  convolutional yaer is often the best solution. Let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "8000/8000 [==============================] - 3716s 464ms/step - loss: 0.3408 - acc: 0.8381 - val_loss: 0.6987 - val_acc: 0.7833\n",
      "Epoch 2/2\n",
      "8000/8000 [==============================] - 4392s 549ms/step - loss: 0.0721 - acc: 0.9737 - val_loss: 1.0910 - val_acc: 0.7865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1102fd710>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeper_classifier = Sequential()\n",
    "deeper_classifier.add(Conv2D(32, (3,3), input_shape=(64, 64, 3), activation='relu'))\n",
    "deeper_classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Add a second convolutionnal layer, input_shape is not necessary because there is other layer before\n",
    "deeper_classifier.add(Conv2D(32, (3,3), activation='relu'))\n",
    "deeper_classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "deeper_classifier.add(Flatten())\n",
    "deeper_classifier.add(Dense(units=128, activation='relu'))\n",
    "deeper_classifier.add(Dense(units=1, activation='sigmoid'))\n",
    "deeper_classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "deeper_classifier.fit_generator(training_set, \n",
    "                         steps_per_epoch=8000, # number of images in training_set\n",
    "                         epochs=2, \n",
    "                         validation_data=test_set, \n",
    "                         validation_steps=2000) # number of images in training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "deeper_classifier.save(\"cnn.h5\")\n",
    "\n",
    "# reload model\n",
    "from keras.models import load_model\n",
    "#deeper_classifier = load_model('cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the class of two pictures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prediction = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))), \n",
    "                          'ressources/Convolutional_Neural_Networks/dataset/single_prediction/')\n",
    "\n",
    "image_1 = image.load_img(path_prediction+'cat_or_dog_1.jpg', target_size=(64, 64))\n",
    "image_1 = image.img_to_array(image_1)\n",
    "image_1 = np.expand_dims(image_1, axis=0)\n",
    "\n",
    "image_2 = image.load_img(path_prediction+'cat_or_dog_2.jpg', target_size=(64, 64))\n",
    "image_2 = image.img_to_array(image_2)\n",
    "image_2 = np.expand_dims(image_2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "prediction = deeper_classifier.predict(image_1)\n",
    "if prediction[0][0] == training_set.class_indices['dogs']:\n",
    "    prediction='dog'\n",
    "else:\n",
    "    prediction='cat'\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
