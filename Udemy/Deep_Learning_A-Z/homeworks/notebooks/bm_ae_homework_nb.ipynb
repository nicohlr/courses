{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning A-Z : Building a Boltzmann Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\">\n",
    "This notebook is my response to the fifth homework of the course called *Deep Learning A-Zâ„¢: Hands-On Artificial Neural Networks* accessible here : https://www.udemy.com/deeplearning/\n",
    "</p>\n",
    "<p align=\"justify\">\n",
    "In this notebook, we are going to build two recommended systems which will tell for each user if this user liked or no a movie. \n",
    "The first recommended system will have a binary output to tell if the user liked or not the movie. We are going to build this recommended system using Boltzmann machines built with pytorch.\n",
    "The second recommended system will output a rating between 1 to five to scale if the user liked or not the movie. We are going to build this recommended system using Autoencoder built with pytorch.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                   1                             2\n",
       "0  1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1  2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2  3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3  4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4  5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath('__file__')))), \n",
    "                          'ressources/Boltzmann_Machines/')\n",
    "\n",
    "movies = pd.read_csv(base_path + 'ml-1m/movies.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
    "users = pd.read_csv(base_path + 'ml-1m/users.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
    "ratings = pd.read_csv(base_path + 'ml-1m/ratings.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
    "\n",
    "movies.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ratings dataset, the first col is the user id, the second is the genre and the third one is the grade. Fourth one is the zip code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1   2   3      4\n",
       "0  1  F   1  10  48067\n",
       "1  2  M  56  16  70072\n",
       "2  3  M  25  15  55117\n",
       "3  4  M  45   7  02460\n",
       "4  5  M  25  20  55455"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ratings dataset, the first col is the user id, the second is the movie that this user rated and the third one is the grade. Fourth one is just a timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1  2          3\n",
       "0  1  1193  5  978300760\n",
       "1  1   661  3  978302109\n",
       "2  1   914  3  978301968\n",
       "3  1  3408  4  978300275\n",
       "4  1  2355  5  978824291"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>5</th>\n",
       "      <th>874965758</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>876893171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>878542960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>876893119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>889751712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>875071561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  1.1  5  874965758\n",
       "0  1    2  3  876893171\n",
       "1  1    3  4  878542960\n",
       "2  1    4  3  876893119\n",
       "3  1    5  3  889751712\n",
       "4  1    7  4  875071561"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cols are the same as for ratings dataset\n",
    "training_set = pd.read_csv(base_path + 'ml-100k/u1.base', delimiter='\\t')\n",
    "test_set = pd.read_csv(base_path + 'ml-100k/u1.test', delimiter='\\t')\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[        1,         2,         3, 876893171],\n",
       "       [        1,         3,         4, 878542960],\n",
       "       [        1,         4,         3, 876893119],\n",
       "       ...,\n",
       "       [      943,      1188,         3, 888640250],\n",
       "       [      943,      1228,         3, 888640275],\n",
       "       [      943,      1330,         3, 888692465]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert data to array\n",
    "training_set = np.array(training_set, dtype='int')\n",
    "test_set = np.array(test_set, dtype='int')\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of users and movies\n",
    "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
    "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the data into an array with users in lines and movies in columns\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        id_movies = data[:,1][data[:,0] == id_users]\n",
    "        id_ratings = data[:,2][data[:,0] == id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data\n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are array with only one data type. So Torch tensors are just multi-dimentionnal arrays exactly as tensorflow tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the data into Torch tensors\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our data are tensors :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 3., 4.,  ..., 0., 0., 0.],\n",
       "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [5., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 5., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Building the Boltzmann Machine for binary outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, as we want to predict binary output, we need to change our input ratings into a binary rating. Indeed, predicted binary rating will be calculated from input rating so it must be binary. Moreover, as 0 currently means that a user doesn't rated a movie, we need to change this value (to -1) because for binary output we need the 0 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[training_set == 0] = -1 # replace 0 by -1 for non rated movies\n",
    "training_set[training_set == 1] = 0\n",
    "training_set[training_set == 2] = 0\n",
    "training_set[training_set >= 3] = 1\n",
    "test_set[test_set == 0] = -1\n",
    "test_set[test_set == 1] = 0\n",
    "test_set[test_set == 2] = 0\n",
    "test_set[test_set >= 3] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM: \n",
    "    \n",
    "    \"\"\"\n",
    "    Create a Bernouilli RBM that will predict if user liked of not a movie, using a binary output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nv, nh):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        nv (int): number of visible nodes\n",
    "        nh (int): number of hidden nodes\n",
    "        \"\"\"\n",
    "        self.Weights = torch.randn(nh, nv) # initialize weights\n",
    "        self.bias_hidden = torch.randn(1, nh) # initialize bias of hidden nodes (fist dimension is batch, second is bias)\n",
    "        self.bias_visible = torch.randn(1, nv) # initialize bias of visible nodes\n",
    "        \n",
    "    def sample_hidden(self, x):\n",
    "        \"\"\"\n",
    "        Function which will activate hidden nodes according to a certain probability given the input nodes. \n",
    "        This probability is the sigmoid function applied to (self.Weights * x + self.bias_hidden).\n",
    "        \"\"\"\n",
    "        wx = torch.mm(x, self.Weights.t()) # make product of two tensors\n",
    "        activation = wx + self.bias_hidden.expand_as(wx) # expand add a new dimention to make sure that bias is applied to each line of the mini batch (arg \"1\")\n",
    "        p_hidden_given_visible = torch.sigmoid(activation) # represents the probability of the hidden node to active given visible node\n",
    "        \n",
    "        # Bernouilli function will create a random number between 0 and 1 and return one if \n",
    "        # this number is higher than p_hidden_given_visible, and 0 else.\n",
    "        \n",
    "        return p_hidden_given_visible, torch.bernoulli(p_hidden_given_visible)\n",
    "    \n",
    "    def sample_visible(self, y):\n",
    "        \"\"\"\n",
    "        Function which will activate visible nodes according to a certain probability given the hidden nodes. \n",
    "        This probability is the sigmoid function applied to (self.Weights * y + self.bias_visible).\n",
    "        \"\"\"\n",
    "        wy = torch.mm(y, self.Weights)\n",
    "        activation = wy + self.bias_visible.expand_as(wy)\n",
    "        p_visible_given_hidden = torch.sigmoid(activation) \n",
    "        \n",
    "        return p_visible_given_hidden, torch.bernoulli(p_visible_given_hidden)\n",
    "    \n",
    "    def train(self, v0, vk, ph0, phk):\n",
    "        \"\"\"\n",
    "        Use contrastive divergence to train our RBM\n",
    "\n",
    "        Args:\n",
    "        v0 : input vector containing ratings\n",
    "        vk : visible nodes obtained after k steps of contrastive divergence\n",
    "        ph0 : initial probability of hidden nodes\n",
    "        phk : probability of hidden nodes after k gibbs samplings\n",
    "        \"\"\"\n",
    "        self.Weights += (torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)).t()\n",
    "\n",
    "        self.bias_visible += torch.sum((v0 - vk), 0)\n",
    "        self.bias_hidden += torch.sum((ph0 - phk), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train function (lines 8/9/10) and epoch training (lines 3/4/5/6) follows this algorithm :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../images/cd.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv = len(training_set[0]) # lenght of fisrt line of training set is the number of inputs\n",
    "nh = 100 # 1682 movies so the model may detect many features, we can start by 100 of them\n",
    "batch_size = 100 # Start with 100 for a fast training (increase diminushes precision)\n",
    "rbm = RBM(nv, nh) # Create restricted Boltzmann machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can try our model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: tensor(0.3346)\n",
      "epoch: 2 loss: tensor(0.2571)\n",
      "epoch: 3 loss: tensor(0.2552)\n",
      "epoch: 4 loss: tensor(0.2504)\n",
      "epoch: 5 loss: tensor(0.2461)\n",
      "epoch: 6 loss: tensor(0.2446)\n",
      "epoch: 7 loss: tensor(0.2499)\n",
      "epoch: 8 loss: tensor(0.2480)\n",
      "epoch: 9 loss: tensor(0.2454)\n",
      "epoch: 10 loss: tensor(0.2480)\n"
     ]
    }
   ],
   "source": [
    "# Training the RBM\n",
    "nb_epoch = 10\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0 # introduce a loss variable which will increase when we will find differences between predictions and answer\n",
    "    counter = 0.\n",
    "    for id_user in range(0, nb_users - batch_size, batch_size):\n",
    "        # create batchs of users\n",
    "        vk = training_set[id_user:id_user+batch_size] # vector that will be the input of gibbs chain and will be updated at each input\n",
    "        v0 = training_set[id_user:id_user+batch_size] \n",
    "        ph0,_ = rbm.sample_hidden(v0)\n",
    "        for k in range(10): # 10 is an hyperparameter, number of random walks\n",
    "            _,hk = rbm.sample_hidden(vk) # do gibbs chain\n",
    "            _,vk = rbm.sample_visible(hk) # do gibbs chain (update of visible nodes)\n",
    "            vk[v0<0] = v0[v0<0] \n",
    "        phk,_ = rbm.sample_hidden(vk)\n",
    "        rbm.train(v0, vk, ph0, phk)\n",
    "        train_loss += torch.mean(torch.abs(v0[v0>=0] - vk[v0>=0])) # update train loss\n",
    "        counter += 1.\n",
    "    print('epoch: ' + str(epoch) + ' loss: ' + str(train_loss/counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: tensor(0.2533)\n"
     ]
    }
   ],
   "source": [
    "# Testing the RBM\n",
    "test_loss = 0\n",
    "counter = 0.\n",
    "for id_user in range(nb_users):\n",
    "    v = training_set[id_user:id_user+1] # we use inputs of training set to activate neurons to predict output for test set\n",
    "    vt = test_set[id_user:id_user+1]\n",
    "    if len(vt[vt>=0]) > 0:\n",
    "        _,h = rbm.sample_hidden(v)\n",
    "        _,v = rbm.sample_visible(h)\n",
    "        test_loss += torch.mean(torch.abs(vt[vt>=0] - v[vt>=0]))\n",
    "        counter += 1.\n",
    "print('test loss: ' + str(test_loss/counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building the Autoencoder for ratings outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to build a Stacked Auto Encoder. The architecture of such model can be found in this figure :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../images/sae.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module): # inheritance from Module class of nn module of pytorch\n",
    "    \n",
    "    \"\"\"\n",
    "    Building a Stacked Auto Encoder\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ):\n",
    "        \n",
    "        super(SAE, self).__init__()\n",
    "        self.full_connection_1 = nn.Linear(nb_movies, 20) # 20 is the number of neurons in hidden layer (number of detected features)\n",
    "        self.full_connection_2 = nn.Linear(20, 10) # we encode the first hidden layer (decrease number of nodes)\n",
    "        self.full_connection_3 = nn.Linear(10, 20) # we decode the second hidden layer (increase number of nodes)\n",
    "        self.full_connection_4 = nn.Linear(20, nb_movies) # ouput layer (with same number of nodes than in input layer)\n",
    "        self.activation = nn.Sigmoid() # sigmoid activation function\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        x : input vector\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.activation(self.full_connection_1(x)) # pass input to fisrt hidden layer\n",
    "        x = self.activation(self.full_connection_2(x)) # pass input to second hidden layer\n",
    "        x = self.activation(self.full_connection_3(x)) # pass input to third hidden layer\n",
    "        x = self.full_connection_4(x) # we do not apply activation function of output layer\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 1.7663763737549565\n",
      "epoch: 2 loss: 1.0964976124193597\n",
      "epoch: 3 loss: 1.0534610503261894\n",
      "epoch: 4 loss: 1.0381563866651102\n",
      "epoch: 5 loss: 1.031060397527553\n",
      "epoch: 6 loss: 1.0264345873198473\n",
      "epoch: 7 loss: 1.0238328781391688\n",
      "epoch: 8 loss: 1.0218807300819088\n",
      "epoch: 9 loss: 1.0210072640670216\n",
      "epoch: 10 loss: 1.0196825133320673\n",
      "epoch: 11 loss: 1.019082204160153\n",
      "epoch: 12 loss: 1.0181814042926405\n",
      "epoch: 13 loss: 1.0182670753373997\n",
      "epoch: 14 loss: 1.017563198485881\n",
      "epoch: 15 loss: 1.0172430537748878\n",
      "epoch: 16 loss: 1.016888068500237\n",
      "epoch: 17 loss: 1.0169046023269215\n",
      "epoch: 18 loss: 1.0164247637942174\n",
      "epoch: 19 loss: 1.016565284026937\n",
      "epoch: 20 loss: 1.0160099571776615\n",
      "epoch: 21 loss: 1.016035854306826\n",
      "epoch: 22 loss: 1.0156174869763015\n",
      "epoch: 23 loss: 1.0158948898161921\n",
      "epoch: 24 loss: 1.015702642954535\n",
      "epoch: 25 loss: 1.0160270450979854\n",
      "epoch: 26 loss: 1.0155900618067224\n",
      "epoch: 27 loss: 1.0156824777170594\n",
      "epoch: 28 loss: 1.0149671533787115\n",
      "epoch: 29 loss: 1.0134353086391739\n",
      "epoch: 30 loss: 1.0117726881547393\n",
      "epoch: 31 loss: 1.010392685761606\n",
      "epoch: 32 loss: 1.0072980842628374\n",
      "epoch: 33 loss: 1.007129411063308\n",
      "epoch: 34 loss: 1.0023553725541987\n",
      "epoch: 35 loss: 1.0023623948928004\n",
      "epoch: 36 loss: 0.9987714151940897\n",
      "epoch: 37 loss: 0.9991597342098076\n",
      "epoch: 38 loss: 0.9977500441780223\n",
      "epoch: 39 loss: 0.9969592388788383\n",
      "epoch: 40 loss: 0.9928250174094752\n",
      "epoch: 41 loss: 0.9935774759216238\n",
      "epoch: 42 loss: 0.9906882954135242\n",
      "epoch: 43 loss: 0.9890087346311165\n",
      "epoch: 44 loss: 0.9840529163361402\n",
      "epoch: 45 loss: 0.9851367306712038\n",
      "epoch: 46 loss: 0.9833534945418729\n",
      "epoch: 47 loss: 0.9821984510244784\n",
      "epoch: 48 loss: 0.9784773879851832\n",
      "epoch: 49 loss: 0.9773562624003123\n",
      "epoch: 50 loss: 0.9756260365011711\n",
      "epoch: 51 loss: 0.9770574097777502\n",
      "epoch: 52 loss: 0.9711311538017344\n",
      "epoch: 53 loss: 0.9713005104179322\n",
      "epoch: 54 loss: 0.9662503219067922\n",
      "epoch: 55 loss: 0.9646701539853593\n",
      "epoch: 56 loss: 0.962430824627871\n",
      "epoch: 57 loss: 0.962233471694593\n",
      "epoch: 58 loss: 0.9606894404911347\n",
      "epoch: 59 loss: 0.9601790593828083\n",
      "epoch: 60 loss: 0.9570269014417556\n",
      "epoch: 61 loss: 0.9573251561247204\n",
      "epoch: 62 loss: 0.9539942087358217\n",
      "epoch: 63 loss: 0.9529586268247712\n",
      "epoch: 64 loss: 0.9513954922476209\n",
      "epoch: 65 loss: 0.952066383292921\n",
      "epoch: 66 loss: 0.9496751610374534\n",
      "epoch: 67 loss: 0.9490846930606952\n",
      "epoch: 68 loss: 0.9470021618835077\n",
      "epoch: 69 loss: 0.9475502126542974\n",
      "epoch: 70 loss: 0.9449753892408138\n",
      "epoch: 71 loss: 0.9459597138193063\n",
      "epoch: 72 loss: 0.9441717379226222\n",
      "epoch: 73 loss: 0.9447448779112841\n",
      "epoch: 74 loss: 0.9424451712984759\n",
      "epoch: 75 loss: 0.943039945052801\n",
      "epoch: 76 loss: 0.9404944676725984\n",
      "epoch: 77 loss: 0.9416786852257754\n",
      "epoch: 78 loss: 0.9402781752506958\n",
      "epoch: 79 loss: 0.9425366208302607\n",
      "epoch: 80 loss: 0.9437823986897115\n",
      "epoch: 81 loss: 0.9429078986541767\n",
      "epoch: 82 loss: 0.9410824771851801\n",
      "epoch: 83 loss: 0.9409187464155738\n",
      "epoch: 84 loss: 0.9388056959117929\n",
      "epoch: 85 loss: 0.9395399040646885\n",
      "epoch: 86 loss: 0.9373013485882982\n",
      "epoch: 87 loss: 0.936859294789388\n",
      "epoch: 88 loss: 0.93637830996222\n",
      "epoch: 89 loss: 0.9360423007748843\n",
      "epoch: 90 loss: 0.9352888681820491\n",
      "epoch: 91 loss: 0.9347315482208359\n",
      "epoch: 92 loss: 0.934674465600927\n",
      "epoch: 93 loss: 0.9337047713995659\n",
      "epoch: 94 loss: 0.9336774226127249\n",
      "epoch: 95 loss: 0.9330199216839261\n",
      "epoch: 96 loss: 0.9328049699080846\n",
      "epoch: 97 loss: 0.9330191530005101\n",
      "epoch: 98 loss: 0.9326577495924111\n",
      "epoch: 99 loss: 0.9317099601697497\n",
      "epoch: 100 loss: 0.9317770031939573\n",
      "epoch: 101 loss: 0.9309951127247905\n",
      "epoch: 102 loss: 0.9306824185862601\n",
      "epoch: 103 loss: 0.9300272763207383\n",
      "epoch: 104 loss: 0.9297846851516702\n",
      "epoch: 105 loss: 0.9293859989430895\n",
      "epoch: 106 loss: 0.929004651033758\n",
      "epoch: 107 loss: 0.9285254367704701\n",
      "epoch: 108 loss: 0.9284732838180585\n",
      "epoch: 109 loss: 0.9288082975895997\n",
      "epoch: 110 loss: 0.9277464164133943\n",
      "epoch: 111 loss: 0.9280508406387387\n",
      "epoch: 112 loss: 0.9272623848064341\n",
      "epoch: 113 loss: 0.9273308327342178\n",
      "epoch: 114 loss: 0.9265467696584946\n",
      "epoch: 115 loss: 0.9269788540634103\n",
      "epoch: 116 loss: 0.925842966626201\n",
      "epoch: 117 loss: 0.9263447466834136\n",
      "epoch: 118 loss: 0.9253463513856619\n",
      "epoch: 119 loss: 0.9257590552456549\n",
      "epoch: 120 loss: 0.9247946094819244\n",
      "epoch: 121 loss: 0.9249794019223144\n",
      "epoch: 122 loss: 0.9242807141215313\n",
      "epoch: 123 loss: 0.9246257972893613\n",
      "epoch: 124 loss: 0.9236797303687311\n",
      "epoch: 125 loss: 0.9238956054838893\n",
      "epoch: 126 loss: 0.9232389120229034\n",
      "epoch: 127 loss: 0.9233942121472672\n",
      "epoch: 128 loss: 0.9226407298872226\n",
      "epoch: 129 loss: 0.9227189918005331\n",
      "epoch: 130 loss: 0.9222656076676881\n",
      "epoch: 131 loss: 0.9220769924414457\n",
      "epoch: 132 loss: 0.9217048546240988\n",
      "epoch: 133 loss: 0.9216637811187623\n",
      "epoch: 134 loss: 0.9212132873592062\n",
      "epoch: 135 loss: 0.9210449024761721\n",
      "epoch: 136 loss: 0.9208121654180504\n",
      "epoch: 137 loss: 0.9205916370016115\n",
      "epoch: 138 loss: 0.9203888096850537\n",
      "epoch: 139 loss: 0.9201552334582128\n",
      "epoch: 140 loss: 0.9199136950559803\n",
      "epoch: 141 loss: 0.9197318079491855\n",
      "epoch: 142 loss: 0.9193920802629919\n",
      "epoch: 143 loss: 0.9192592268656817\n",
      "epoch: 144 loss: 0.9192488988177799\n",
      "epoch: 145 loss: 0.918920589058621\n",
      "epoch: 146 loss: 0.9186134160362542\n",
      "epoch: 147 loss: 0.9179808990072481\n",
      "epoch: 148 loss: 0.9181454237920642\n",
      "epoch: 149 loss: 0.9181314749131133\n",
      "epoch: 150 loss: 0.9177577004240106\n",
      "epoch: 151 loss: 0.9176605024613357\n",
      "epoch: 152 loss: 0.9174894576602696\n",
      "epoch: 153 loss: 0.9173635727051259\n",
      "epoch: 154 loss: 0.9172695240109077\n",
      "epoch: 155 loss: 0.9168932490816291\n",
      "epoch: 156 loss: 0.9166924819077562\n",
      "epoch: 157 loss: 0.9164824566723503\n",
      "epoch: 158 loss: 0.9163172349289448\n",
      "epoch: 159 loss: 0.9162840005477695\n",
      "epoch: 160 loss: 0.9161620466792141\n",
      "epoch: 161 loss: 0.9162061742146452\n",
      "epoch: 162 loss: 0.9160808771989111\n",
      "epoch: 163 loss: 0.9161509429167805\n",
      "epoch: 164 loss: 0.9156200989086032\n",
      "epoch: 165 loss: 0.9155275051353435\n",
      "epoch: 166 loss: 0.9152205579172245\n",
      "epoch: 167 loss: 0.9150278179643324\n",
      "epoch: 168 loss: 0.9148956084475186\n",
      "epoch: 169 loss: 0.9148816341375186\n",
      "epoch: 170 loss: 0.9147289319565917\n",
      "epoch: 171 loss: 0.9148766268128165\n",
      "epoch: 172 loss: 0.9146442713891002\n",
      "epoch: 173 loss: 0.9144215155650108\n",
      "epoch: 174 loss: 0.9142249334296422\n",
      "epoch: 175 loss: 0.9142163679681389\n",
      "epoch: 176 loss: 0.913661648905231\n",
      "epoch: 177 loss: 0.9139027469722395\n",
      "epoch: 178 loss: 0.9135981030493984\n",
      "epoch: 179 loss: 0.9136942498961357\n",
      "epoch: 180 loss: 0.9132620359879587\n",
      "epoch: 181 loss: 0.9134131737579744\n",
      "epoch: 182 loss: 0.9130656915816169\n",
      "epoch: 183 loss: 0.9133068658762905\n",
      "epoch: 184 loss: 0.9128521898549883\n",
      "epoch: 185 loss: 0.9129798973017437\n",
      "epoch: 186 loss: 0.9125268371335243\n",
      "epoch: 187 loss: 0.912803648879147\n",
      "epoch: 188 loss: 0.9121670646420387\n",
      "epoch: 189 loss: 0.9127131034023575\n",
      "epoch: 190 loss: 0.9121243458207999\n",
      "epoch: 191 loss: 0.9122950172338905\n",
      "epoch: 192 loss: 0.9117690571779169\n",
      "epoch: 193 loss: 0.9121965102380654\n",
      "epoch: 194 loss: 0.9116554644222046\n",
      "epoch: 195 loss: 0.9119597910740884\n",
      "epoch: 196 loss: 0.9117626591319292\n",
      "epoch: 197 loss: 0.9118494765577553\n",
      "epoch: 198 loss: 0.911305286731326\n",
      "epoch: 199 loss: 0.9116297927404811\n",
      "epoch: 200 loss: 0.9111463939045835\n"
     ]
    }
   ],
   "source": [
    "# Training the SAE\n",
    "nb_epoch = 200\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    counter = 0. # float to avoid warning\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0) # add additinnal dimension corresponding to the batch (0 is index of new dimension)\n",
    "        target = input.clone() # target is the same as input vector for Auto Encoder \n",
    "        if torch.sum(target.data > 0) > 0: # optimize code for bigger datasets (goes in loof only if at least one user rated one movie)\n",
    "            output = sae(input) # use Auto Encoder\n",
    "            target.require_grad = False # compute gradient only with respect to the input and not with respect to the target\n",
    "            output[target == 0] = 0 # save up some memory for bigger datasets\n",
    "            loss = criterion(output, target) # calculate loss between output and target\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) # number of movies over number of movies that habe positive rating (+1e10 to avoid zero division without adding bias)\n",
    "            loss.backward() # indicate if needs to increase or decrease the weights using backward function\n",
    "            train_loss += np.sqrt(loss.item()*mean_corrector) # update loss value\n",
    "            counter += 1.\n",
    "            optimizer.step() # apply the optimizer to change the weights (amount of change in the weights)\n",
    "    print('epoch: ' + str(epoch) + ' loss: ' +  str(train_loss/counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.9533672705973602\n"
     ]
    }
   ],
   "source": [
    "# Testing the SAE\n",
    "test_loss = 0\n",
    "counter = 0.\n",
    "for id_user in range(nb_users):\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0) # we use training ratings to predict ratings for test set\n",
    "    target = Variable(test_set[id_user]).unsqueeze(0) # what we want to predict\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = sae(input)\n",
    "        target.require_grad = False\n",
    "        output[target == 0] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.item()*mean_corrector)\n",
    "        counter += 1.\n",
    "print('test loss: ' + str(test_loss/counter))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
